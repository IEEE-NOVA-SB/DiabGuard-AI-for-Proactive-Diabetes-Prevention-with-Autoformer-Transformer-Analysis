{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1133fc-2531-4102-bfce-89a213f143e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoformerConfig, AutoformerModel\n",
    "import optuna\n",
    "\n",
    "# Fetch dataset\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e670a88-79c6-4acd-9142-d78660eaa339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = fetch_ucirepo(id=296)\n",
    "X, y = dataset.data.features, dataset.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0d1da-fa69-427f-8e98-870f6d8082e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2d01d-aa91-4d7c-a952-bf933b507846",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac6812f-05ee-44a5-9716-a4080b62c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN in numerical columns with the median and in categorical columns with 'Unknown'\n",
    "for col in X.columns:\n",
    "    if pd.api.types.is_numeric_dtype(X[col]):\n",
    "        X[col].fillna(X[col].median(), inplace=True)\n",
    "    elif pd.api.types.is_object_dtype(X[col]) or pd.api.types.is_categorical_dtype(X[col]):\n",
    "        X[col].fillna('Unknown', inplace=True)  # Or use mode: X[col].fillna(X[col].mode()[0], inplace=True)\n",
    "\n",
    "# Encode categorical features using LabelEncoder\n",
    "label_encoders = {col: LabelEncoder().fit(X[col]) for col in X.select_dtypes(include=['object', 'category']).columns}\n",
    "X = X.apply(lambda col: label_encoders[col.name].transform(col) if col.name in label_encoders else col)\n",
    "\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8d1a4-152a-45fa-8f82-5216c0e918d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'y' is a DataFrame with a single target column\n",
    "column_name = y.columns[0]  # Dynamically get the name of the column\n",
    "\n",
    "# Encode the target column if it's categorical\n",
    "if y[column_name].dtype == 'object' or y[column_name].dtype.name == 'category':\n",
    "    le_y = LabelEncoder()\n",
    "    y[column_name] = le_y.fit_transform(y[column_name].astype(str))\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81471ea-3a0f-46ec-b1b7-ef586fbe2ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset and scaling features\n",
    "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ce755-5e22-44cf-bc4d-660ed0828a61",
   "metadata": {},
   "source": [
    "## Codigo com linhas de codigo mais pequenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df901247-eaa5-4487-9e72-bfe639d78795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32) if isinstance(X_train_scaled, pd.DataFrame) else torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_encoded.values, dtype=torch.long) if isinstance(y_train_encoded, pd.DataFrame) else torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32) if isinstance(X_test_scaled, pd.DataFrame) else torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_encoded.values, dtype=torch.long) if isinstance(y_test_encoded, pd.DataFrame) else torch.tensor(y_test_encoded, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c29fa16-c240-4899-b26b-a85e9b2ef8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False) # Typically, you don't need to shuffle the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b4d8f-bce9-4baa-aa39-6097381e6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a default Autoformer configuration\n",
    "configuration = AutoformerConfig(\n",
    "    # Add your model-specific configurations here\n",
    "    prediction_length=10,  # Example: predict 10 time steps into the future\n",
    "    context_length=30,  # Example: look at 30 time steps in the past\n",
    "    # You might need to adjust other parameters depending on your dataset and requirements\n",
    "    feature_size=47,  # Assuming you have 47 features\n",
    ")\n",
    "# Initializing a model from the configuration\n",
    "model = AutoformerModel(configuration)\n",
    "\n",
    "# Accessing the model configuration if needed\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3e54a-4111-42e1-ae3d-b52bb2c0b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoformerClassifier(nn.Module):\n",
    "    def __init__(self, autoformer_model, num_features, num_classes):\n",
    "        super(AutoformerClassifier, self).__init__()\n",
    "        self.autoformer = autoformer_model  # The loaded Autoformer model\n",
    "        self.classifier = nn.Linear(num_features, num_classes)  # Classification layer\n",
    "\n",
    "    def forward(self, past_time_features, past_observed_mask):\n",
    "        # Adjust the forward method to accept past_time_features and past_observed_mask\n",
    "        outputs = self.autoformer(past_time_features=past_time_features, past_observed_mask=past_observed_mask)\n",
    "        \n",
    "        # Assuming we are interested in the last timestep's output for classification\n",
    "        # You might need to adapt this depending on your specific use case and what the model outputs\n",
    "        # For instance, outputs.last_hidden_state might not be directly applicable depending on how Autoformer's output is structured\n",
    "        last_hidden_state = outputs.last_hidden_state if hasattr(outputs, 'last_hidden_state') else outputs[0]\n",
    "        logits = self.classifier(last_hidden_state[:, -1, :])\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed8a23-5f9e-441f-9c4e-80adea6e09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, learning_rate, X_train, mask_train, y_train, X_val, epochs=3):\n",
    "    model.train()  # Set the model to training mode\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training loop\n",
    "        for X_batch, mask_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(past_time_features=X_batch, past_observed_mask=mask_batch)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total, correct = 0, 0\n",
    "        with torch.no_grad():  # No gradients needed for validation\n",
    "            for X_batch, mask_batch, y_batch in val_loader:\n",
    "                outputs = model(past_time_features=X_batch, past_observed_mask=mask_batch)\n",
    "                _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        validation_accuracy = correct / total\n",
    "        print(f'Epoch {epoch+1}, Validation Accuracy: {validation_accuracy:.4f}')\n",
    "    \n",
    "    return validation_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e068c4a-51bd-4ef6-9c88-ee5199acf24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    \n",
    "    # Assuming the datasets are tensors already, otherwise, convert them\n",
    "    validation_accuracy = train_and_evaluate(model, learning_rate, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    return validation_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b9f0c-de6a-420a-ae99-80ecc7973142",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)  # Adjust n_trials as needed based on computational resources and needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e73f5ce-1412-42b6-82b8-c4043df2b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best trial: {study.best_trial.params}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
